import os
import json
import torch
import numpy as np
from torch.utils.data import Dataset
from typing import Optional, Callable, Dict
from tqdm import tqdm
from multiprocessing import Pool

# Assuming these are defined elsewhere as in the original code.
# You may need to adjust the import paths based on your project structure.
from mri_ct_diffusion.config import (
    MODEL_DIM,
    ORIGINAL_DATA_ROOT,
    PREPROCESSED_DATA_ROOT,
    SPLITS_DIR,
    TRAINING_NUM_WORKERS,
    INFERENCE_NUM_WORKERS,
)
from mri_ct_diffusion.preprocess import process_patient
from mri_ct_diffusion.preprocessing.utils import get_patient_ids, load_patient_volumes
from mri_ct_diffusion.preprocessing.stats import load_global_stats


def _process_patient_worker(args):
    """
    Worker function to process a single patient's data.
    This function is designed to be called by a multiprocessing pool.
    """
    patient_id, use_preprocessed, root, mode, patient_exclusion_info, file_ext = args

    slice_exclusion_log = []

    try:
        # This function is called within a worker, so we can't use process_patient which creates a new Pool
        if not use_preprocessed:
            # This path is problematic for multiprocessing.
            # The main class will handle this by falling back to sequential loading.
            process_patient(patient_id, root)

        ct_vol, mr_vol, _ = load_patient_volumes(root, patient_id, file_ext=file_ext)

        if mr_vol is None or ct_vol is None:
            return patient_id, [], [f"‚ö†Ô∏è Skipping patient {patient_id} due to missing MR or CT volume."], False

        if mr_vol.ndim == 3:
            mr_vol = np.expand_dims(mr_vol, axis=-1)
        if ct_vol.ndim == 3:
            ct_vol = np.expand_dims(ct_vol, axis=-1)

        mr_vol_t = torch.from_numpy(mr_vol).permute(3, 0, 1, 2).float()
        ct_vol_t = torch.from_numpy(ct_vol).permute(3, 0, 1, 2).float()

        num_slices = mr_vol_t.shape[1]
        excluded_slices = set(patient_exclusion_info.get("excluded_slices", []))

        patient_slices = []
        for s_idx in range(num_slices):
            if s_idx in excluded_slices:
                msg = f"‚ùå Skipping slice {s_idx} for patient {patient_id}"
                slice_exclusion_log.append(msg)
                continue

            mr_slice = mr_vol_t[:, s_idx, :, :]

            sample = {
                "mri": mr_slice,
                "patient_id_str": patient_id,  # Use string ID temporarily
                "slice_idx": s_idx,
            }

            if mode in ["train", "val", 'infer']:
                ct_slice = ct_vol_t[:, s_idx, :, :]
                sample["ct"] = ct_slice

            patient_slices.append(sample)

        return patient_id, patient_slices, slice_exclusion_log, True

    except Exception as e:
        return patient_id, [], [f"‚ö†Ô∏è Error processing patient {patient_id}, skipping. Error: {e}"], False


class MRISCTDataset(Dataset):
    """
    MRI-SCT Dataset.

    This version loads all individual 2D slices into memory during initialization
    for faster access during training and evaluation. __getitem__ retrieves a
    pre-loaded slice directly from memory.
    It can use multiple workers to speed up the initial loading and caching.
    """

    def __init__(
        self,
        mode: str = "train",
        transform: Optional[Callable] = None,
        use_preprocessed: bool = True,
        allowed_patients: Optional[list] = None,
        num_workers: Optional[int] = None,
    ):
        """
        Initializes the dataset by loading and caching all required slices.

        Args:
            mode (str): Dataset mode, one of ["train", "val", "infer", "challenge"].
            transform (Optional[Callable]): A function/transform to apply to the data.
            use_preprocessed (bool): Whether to use preprocessed data.
            allowed_patients (Optional[list]): A specific list of patient IDs to load.
            num_workers (Optional[int]): Number of worker processes to use for data loading.
                                         If None, defaults based on mode from config.
        """
        assert mode in ["train", "val", "infer", "challenge"], "Invalid mode"
        if MODEL_DIM != "2D":
            raise NotImplementedError(f"This Dataset implementation currently only supports MODEL_DIM='2D'.")

        self.mode = mode
        self.transform = transform
        self.use_preprocessed = use_preprocessed
        self.root = PREPROCESSED_DATA_ROOT if self.use_preprocessed else ORIGINAL_DATA_ROOT

        if num_workers is None:
            if self.mode == "train":
                self.num_workers = TRAINING_NUM_WORKERS
            else:  # val, infer, challenge
                self.num_workers = INFERENCE_NUM_WORKERS
        else:
            self.num_workers = num_workers

        self.slices = []
        self.patient_ids_loaded = []
        #self.stats = load_global_stats()
        self.exclusion_dict = {}
        self.slice_exclusion_log = []

        if self.mode == "train":
            exclusion_path = os.path.join(SPLITS_DIR, "mr.mha-comment_original.json")
            if os.path.exists(exclusion_path):
                with open(exclusion_path, "r") as f:
                    self.exclusion_dict = json.load(f)
            else:
                print(f"‚ö†Ô∏è Exclusion file not found at: {exclusion_path}. No slices will be excluded.")

        if mode in ["train", "val"]:
            split_file = os.path.join(SPLITS_DIR, f"{mode}.txt")
            if not os.path.exists(split_file):
                raise FileNotFoundError(f"Missing split file: {split_file}")
            with open(split_file) as f:
                self.allowed_patients = set(line.strip() for line in f if line.strip())
        else:
            self.allowed_patients = set(allowed_patients) if allowed_patients else None

        self._load_and_cache_slices()

        print(f"\nüìÇ [{self.mode}] Loaded data for {len(self.patient_ids_loaded)} patients from {self.root}")
        if self.mode in ["train", "val"]:
            print(f"üóûÔ∏è [{self.mode}] Patient IDs used: {sorted(self.patient_ids_loaded)}")
            os.makedirs("logs", exist_ok=True)
            with open(f"logs/used_patients_{self.mode}.txt", "w") as f:
                for pid in sorted(self.patient_ids_loaded):
                    f.write(f"{pid}\n")

            if self.mode == "train" and self.slice_exclusion_log:
                with open("logs/excluded_slices_train.txt", "w") as f:
                    for line in self.slice_exclusion_log:
                        f.write(f"{line}\n")

        print(f"üëÅÔ∏è [{self.mode}] Total usable slices pre-loaded into memory: {len(self.slices)}")

    def _load_and_cache_slices(self):
        """
        Dispatches loading to parallel or sequential implementation based on settings.
        """
        file_ext = "npy" if self.use_preprocessed else "nii"
        print(f"üîç Loading and caching all slices from: {self.root} (use_preprocessed={self.use_preprocessed})")

        patient_ids_to_process = get_patient_ids(self.root, file_ext=file_ext)

        eligible_patient_ids = []
        for patient_id in patient_ids_to_process:
            if self.allowed_patients is not None and patient_id not in self.allowed_patients:
                continue

            if self.mode == "train" and patient_id in self.exclusion_dict:
                info = self.exclusion_dict.get(patient_id, {})
                if not info.get("include", True):
                    log_msg = f"üö´ Skipping entire patient (marked for exclusion): {patient_id}"
                    self.slice_exclusion_log.append(log_msg)
                    continue
            eligible_patient_ids.append(patient_id)

        if not self.use_preprocessed or self.num_workers <= 1:
            print(f"üêå Using sequential loading (use_preprocessed={self.use_preprocessed}, num_workers={self.num_workers})")
            self._load_and_cache_slices_sequential(eligible_patient_ids)
        else:
            print(f"üöÄ Using parallel loading with {self.num_workers} workers.")
            self._load_and_cache_slices_parallel(eligible_patient_ids)

        # Final step: Remap string patient IDs to integer indices for all slices
        if self.slices:
            self.patient_ids_loaded = sorted(list(set(pid for s in self.slices for pid in [s["patient_id_str"]])))
            pid_to_v_idx = {pid: i for i, pid in enumerate(self.patient_ids_loaded)}

            for s in self.slices:
                s["patient_idx"] = pid_to_v_idx[s.pop("patient_id_str")]

    def _load_and_cache_slices_sequential(self, patient_ids_to_process):
        """
        Original sequential implementation for loading slices.
        """
        file_ext = "npy" if self.use_preprocessed else "nii"
        for patient_id in tqdm(patient_ids_to_process, desc="Loading and Caching Slices (Sequential)", leave=False):
            try:
                if not self.use_preprocessed:
                    process_patient(patient_id, self.root)

                ct_vol, mr_vol, _ = load_patient_volumes(self.root, patient_id, file_ext=file_ext)

                if mr_vol is None or ct_vol is None:
                    print(f"‚ö†Ô∏è Skipping patient {patient_id} due to missing MR or CT volume.")
                    continue

                if mr_vol.ndim == 3: mr_vol = np.expand_dims(mr_vol, axis=-1)
                if ct_vol.ndim == 3: ct_vol = np.expand_dims(ct_vol, axis=-1)

                mr_vol_t = torch.from_numpy(mr_vol).permute(3, 0, 1, 2).float()
                ct_vol_t = torch.from_numpy(ct_vol).permute(3, 0, 1, 2).float()

                num_slices = mr_vol_t.shape[1]
                excluded_slices = set()
                if self.mode == "train" and patient_id in self.exclusion_dict:
                    excluded_slices = set(self.exclusion_dict[patient_id].get("excluded_slices", []))

                for s_idx in range(num_slices):
                    if s_idx in excluded_slices:
                        msg = f"‚ùå Skipping slice {s_idx} for patient {patient_id}"
                        self.slice_exclusion_log.append(msg)
                        continue

                    mr_slice = mr_vol_t[:, s_idx, :, :]
                    sample = {
                        "mri": mr_slice,
                        "patient_id_str": patient_id,
                        "slice_idx": s_idx,
                    }

                    if self.mode in ["train", "val", 'infer']:
                        ct_slice = ct_vol_t[:, s_idx, :, :]
                        sample["ct"] = ct_slice

                    self.slices.append(sample)

            except Exception as e:
                print(f"‚ö†Ô∏è Error processing patient {patient_id}, skipping. Error: {e}")

    def _load_and_cache_slices_parallel(self, patient_ids_to_process):
        """
        Parallel implementation for loading slices using multiprocessing.
        """
        file_ext = "npy" if self.use_preprocessed else "nii"

        worker_args = []
        for patient_id in patient_ids_to_process:
            patient_exclusion_info = self.exclusion_dict.get(patient_id, {}) if self.mode == "train" else {}
            worker_args.append(
                (patient_id, self.use_preprocessed, self.root, self.mode, patient_exclusion_info, file_ext)
            )

        with Pool(processes=self.num_workers) as pool:
            pbar = tqdm(
                pool.imap_unordered(_process_patient_worker, worker_args),
                total=len(worker_args),
                desc="Loading and Caching Slices (Parallel)",
            )
            for result in pbar:
                _, patient_slices, log_messages, success = result
                self.slice_exclusion_log.extend(log_messages)
                if success:
                    self.slices.extend(patient_slices)

    def __len__(self) -> int:
        """Returns the total number of usable slices."""
        return len(self.slices)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Returns a pre-loaded data sample from memory.
        """
        sample = self.slices[idx]
        # if self.transform:
        #     sample = self.transform(sample)
        return sample
